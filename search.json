[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Explainax: An Explainable AI Library for JAX",
    "section": "",
    "text": "Installation | Documentation\nexplainax is a JAX library for model explanation and interpretation.",
    "crumbs": [
      "Explainax: An Explainable AI Library for JAX"
    ]
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "Explainax: An Explainable AI Library for JAX",
    "section": "Installation",
    "text": "Installation\nSimply install via pypi:\npip install explainax --upgrade\nOr upgrade to the latest version of explainax:\npip install git+https://github.com/BirkhoffG/explainax.git --upgrade",
    "crumbs": [
      "Explainax: An Explainable AI Library for JAX"
    ]
  },
  {
    "objectID": "api/linear_model.html",
    "href": "api/linear_model.html",
    "title": "Linear Model",
    "section": "",
    "text": "x = jrand.normal(jrand.PRNGKey(0), (100, 2))\n\n\nsource\n\nl2_loss\n\n l2_loss (x1, x2, weights=None)\n\n\nsource\n\n\nsgd_train_linear_model\n\n sgd_train_linear_model (X:jax.Array, y:jax.Array, weights:jax.Array=None,\n                         lr:float=0.01, n_epochs:int=100,\n                         batch_size:int=32, seed:int=42,\n                         loss_fn:Callable=&lt;function l2_loss&gt;,\n                         reg_term:int=None, alpha:float=1.0,\n                         fit_bias:bool=True)\n\nTrain a linear model using SGD.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nX\njnp.ndarray\n\nInput data. Shape: (N, k)\n\n\ny\njnp.ndarray\n\nTarget data. Shape: (N,) or (N, 1)\n\n\nweights\njnp.ndarray\nNone\nInitial weights. Shape: (N,)\n\n\nlr\nfloat\n0.01\nLearning rate\n\n\nn_epochs\nint\n100\nNumber of epochs\n\n\nbatch_size\nint\n32\nBatch size\n\n\nseed\nint\n42\nRandom seed\n\n\nloss_fn\nCallable\nl2_loss\nLoss function\n\n\nreg_term\nint\nNone\nRegularization term\n\n\nalpha\nfloat\n1.0\nRegularization strength\n\n\nfit_bias\nbool\nTrue\nFit bias term\n\n\nReturns\nTuple[np.ndarray, np.ndarray]\n\nThe trained weights and bias\n\n\n\n\nsource\n\n\ncalculate_loss\n\n calculate_loss (params:Dict[str,jax.Array],\n                 batch:Tuple[jax.Array,jax.Array,jax.Array],\n                 loss_fn:Callable, reg_term:int=None, alpha:float=1.0)\n\nCalculate the loss for a batch of data.\n\nsource\n\n\nBaseEstimator\n\n BaseEstimator ()\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nLinearModel\n\n LinearModel (intercept:bool=True, trainer_fn:Callable=None, **kwargs)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nLasso\n\n Lasso (alpha:float=1.0, **kwargs)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nRidge\n\n Ridge (alpha:float=1.0, **kwargs)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nTest\n\nfrom sklearn.datasets import make_regression\nfrom sklearn.linear_model import LinearRegression\n\n\nX, y = make_regression(n_samples=500, n_features=20)\nw = np.ones(X.shape[0])\n\n\nsk_lm = LinearRegression()\nsk_lm.fit(X, y)\nsk_lm.coef_, sk_lm.intercept_\n\n(array([ 4.39208757e+01,  5.56077362e+01,  8.41533489e+01, -1.96221110e-15,\n        -3.12695418e-14,  2.31507874e-14, -3.24155956e-14,  2.37743954e+00,\n         2.76497270e+01, -1.59962711e-15,  1.29899749e-14, -4.45028592e-14,\n        -4.98098774e-14, -8.17722613e-14,  9.44247846e+01,  8.93984093e+01,\n         5.23727300e+01, -8.15458812e-14,  7.31298648e+01,  4.14151921e+00]),\n -1.0658141036401503e-14)\n\n\n\nlm = LinearModel()\nlm.fit(X, y)\nlm.fit(X, y, w)\nlm.coef_, lm.intercept_\n\n(Array([ 4.3920807e+01,  5.5607498e+01,  8.4153252e+01,  2.5172596e-04,\n        -2.4921859e-05, -9.8353492e-05, -2.5988952e-04,  2.3774581e+00,\n         2.7649561e+01,  3.1019867e-04, -2.3504299e-04,  2.3154756e-04,\n         9.7808908e-05, -6.8332774e-05,  9.4424484e+01,  8.9398209e+01,\n         5.2372467e+01, -2.0462631e-04,  7.3129379e+01,  4.1418076e+00],      dtype=float32),\n Array([1.1218661e-05], dtype=float32))\n\n\n\nassert np.allclose(sk_lm.coef_, lm.coef_, atol=5e-4)\nassert np.allclose(sk_lm.intercept_, lm.intercept_, atol=5e-4)\n\n\nlasso = Lasso(alpha=0.1)\nlasso.fit(X, y)\nlasso.fit(X, y, w)\nlasso.coef_, lasso.intercept_\n\n(Array([ 4.3809612e+01,  5.5486099e+01,  8.4031708e+01, -2.9320540e-04,\n         4.6559394e-04, -9.0057432e-04,  1.1306580e-03,  2.2790406e+00,\n         2.7526842e+01, -1.0292386e-03, -5.8906851e-04,  1.5921631e-03,\n        -1.3546057e-03, -5.7649292e-04,  9.4333282e+01,  8.9276718e+01,\n         5.2267292e+01,  3.2355968e-04,  7.2997238e+01,  4.0658412e+00],      dtype=float32),\n Array([0.00556847], dtype=float32))\n\n\n\nridge = Ridge(alpha=0.1)\nridge.fit(X, y)\nridge.fit(X, y, w)\nridge.coef_, ridge.intercept_\n\n(Array([ 4.3895161e+01,  5.5568810e+01,  8.4103256e+01,  1.9272733e-03,\n        -1.3708844e-03, -2.7952294e-04, -5.8420287e-03,  2.3761270e+00,\n         2.7629921e+01,  8.9110909e-03, -1.1335424e-03,  3.1227635e-03,\n         1.2426455e-04, -9.2288008e-04,  9.4378990e+01,  8.9345421e+01,\n         5.2339722e+01, -6.7419285e-04,  7.3078979e+01,  4.1462922e+00],      dtype=float32),\n Array([0.00056191], dtype=float32))",
    "crumbs": [
      "API",
      "Linear Model"
    ]
  },
  {
    "objectID": "api/attr.base.html",
    "href": "api/attr.base.html",
    "title": "Base API",
    "section": "",
    "text": "source\n\nAttribution\n\n Attribution (func:Callable, additional_func_args:Dict=None)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfunc\ntyping.Callable\n\nA black-box function to be explained\n\n\nadditional_func_args\ntyping.Dict\nNone\nAdditional arguments for the black-box function",
    "crumbs": [
      "API",
      "Base API"
    ]
  },
  {
    "objectID": "api/attr.lime.html",
    "href": "api/attr.lime.html",
    "title": "LIME",
    "section": "",
    "text": "source\n\npairwise_distances\n\n pairwise_distances (x:jax.Array, y:jax.Array, metric:str='euclidean')\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nx\nArray\n\n[n, k]\n\n\ny\nArray\n\n[m, k]\n\n\nmetric\nstr\neuclidean\nSupports “euclidean” and “cosine”\n\n\nReturns\nArray\n\n[n, m]\n\n\n\nThis function is similar to sklearn.metrics.pairwise_distances.\n\nfrom sklearn.metrics import pairwise_distances as sk_pairwise_distances\n\npairwise_distances is faster than sklearn’s implementation.\n\nX = np.random.normal(size=(1000, 28 * 28))\nY = np.random.normal(size=(1000, 28 * 28))\n\ndef benchmark_pairwise_distances(metric):\n    print(f\"[{metric}] Sklearn pairwise_distances:\")\n\n    print(f\"[{metric}] JAX pairwise_distances:\")\n\n    assert jnp.allclose(\n        sk_pairwise_distances(X, Y, metric=metric),\n        pairwise_distances(X, Y, metric=metric)\n    )\n\nbenchmark_pairwise_distances(\"euclidean\")\nbenchmark_pairwise_distances(\"cosine\")\n\n[euclidean] Sklearn pairwise_distances:\n28.6 ms ± 6.82 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n[euclidean] JAX pairwise_distances:\n6.27 ms ± 3.02 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n[cosine] Sklearn pairwise_distances:\n29.2 ms ± 6.07 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n[cosine] JAX pairwise_distances:\n6.4 ms ± 2.82 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\n\nsource\n\n\ngaussian_perturb_func\n\n gaussian_perturb_func (x:jax.Array, prng_key:&lt;function PRNGKey&gt;,\n                        **kwargs)\n\nGaussian perturbation function for LIME\n\nsource\n\n\nbernoulli_perturb_func\n\n bernoulli_perturb_func (x:jax.Array, prng_key:&lt;function PRNGKey&gt;,\n                         **kwargs)\n\nBernoulli perturbation function for LIME\n\nX = np.random.normal(size=(1, 28 * 28))\nb_perturbed = _perturb_data(X, 100, bernoulli_perturb_func, jrand.PRNGKey(42))\ng_perturbed = _perturb_data(X, 100, gaussian_perturb_func, jrand.PRNGKey(42))\nassert b_perturbed.shape == (101, 28 * 28)\nassert g_perturbed.shape == (101, 28 * 28)\n\n\nsource\n\n\nexp_kernel_func\n\n exp_kernel_func (dists:jax.Array, kernel_width:float)\n\nExponential kernel function for LIME\n\ndistances = pairwise_distances(g_perturbed, X)\n\nThe slowest run took 389.12 times longer than the fastest. This could mean that an intermediate result is being cached.\n273 µs ± 657 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\n\nsource\n\n\nLimeBase\n\n LimeBase (func:Callable, additional_func_args:Dict=None,\n           model_regressor=None, kernal_func:Callable=None,\n           kernel_width:float=None, perturb_func:Callable=None,\n           input_paramter_name:str='x',\n           pairwise_distances_metric:str='euclidean')\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfunc\ntyping.Callable\n\nA black-box function to be explained\n\n\nadditional_func_args\ntyping.Dict\nNone\nAdditional arguments for the black-box function\n\n\nmodel_regressor\nNoneType\nNone\nLinear regressor to use in explanation\n\n\nkernal_func\ntyping.Callable\nNone\nKernel function for computing similarity\n\n\nkernel_width\nfloat\nNone\nKernel width for computing similarity. Defaults to (n_features * 0.75)\n\n\nperturb_func\ntyping.Callable\nNone\nPerturbation function for generating perturbed instances\n\n\ninput_paramter_name\nstr\nx\nName of the input parameter for the black-box function\n\n\npairwise_distances_metric\nstr\neuclidean\nMetric for computing pairwise distances\n\n\n\n\nTests\n\nfrom sklearn.datasets import make_regression\nimport haiku as hk\n\n\nxs, ys = make_regression(n_samples=500, n_features=20)\n\n\nlinear_model = LinearModel()\nlinear_model.fit(xs, ys)\nlime = LimeBase(linear_model.predict, input_paramter_name=\"X\")\n\n\n# fit a simple haiku model\ndef model(x):\n    mlp = hk.Sequential([\n        hk.Linear(10),\n        jax.nn.relu,\n        hk.Linear(10),\n        jax.nn.relu,\n        hk.Linear(1),\n    ])\n    return mlp(x)\n\n\ndef init(x):\n    net = hk.without_apply_rng(hk.transform(model))\n    opt = optax.sgd(1e-1)\n    params = net.init(jrand.PRNGKey(42), x)\n    opt_state = opt.init(params)\n    return net, opt, params, opt_state\n\ndef loss(params, net, x, y):\n    pred = net.apply(params, x)\n    return jnp.mean((pred - y) ** 2)\n\n@partial(jax.jit, static_argnums=(2,3))\ndef update(\n    params: hk.Params,\n    opt_state: optax.OptState,\n    net: hk.Transformed,\n    opt: optax.GradientTransformation,\n    x: jnp.ndarray,\n    y: jnp.ndarray\n):\n    grads = jax.grad(loss)(params, net, x, y)\n    updates, opt_state = opt.update(grads, opt_state)\n    params = optax.apply_updates(params, updates)\n    return params, opt_state\n\ndef train(\n    net: hk.Transformed,\n    opt: optax.GradientTransformation,\n    params: hk.Params,\n    opt_state: optax.OptState,\n    x: jnp.ndarray,\n    y: jnp.ndarray,\n    n_epochs: int = 100,\n    batch_size: int = 32,\n):\n    n_samples = x.shape[0]\n    for _ in range(n_epochs):\n        for i in range(0, n_samples, batch_size):\n            x_batch = x[i:i+batch_size]\n            y_batch = y[i:i+batch_size]\n            params, opt_state = update(params, opt_state, net, opt, x_batch, y_batch)\n    return params\n\ndef fit_a_model(\n    X: Array,\n    y: Array,\n):\n    net, opt, params, opt_state = init(X)\n    params = train(net, opt, params, opt_state, X, y)\n    return net, params\n\n\nnet, params = fit_a_model(xs, ys)\n\n/home/birk/mambaforge-pypy3/envs/nbdev2/lib/python3.7/site-packages/haiku/_src/base.py:515: UserWarning: Explicitly requested dtype float64 requested in zeros is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n  param = init(shape, dtype)\n\n\n\nnet.apply(params, xs[:10])\n\nDeviceArray([[0.24004635],\n             [0.24004635],\n             [0.24004635],\n             [0.24004635],\n             [0.24004635],\n             [0.24004635],\n             [0.24004635],\n             [0.24004635],\n             [0.24004635],\n             [0.24004635]], dtype=float32)\n\n\n\nkernel_func = partial(exp_kernel_func, kernel_width=2 * 0.75)\n\n_lime_attribute_single_instance(\n    X[:1],\n    1000,\n    jrand.PRNGKey(42),\n    net.apply,\n    additional_func_args={\"params\": params},\n    input_paramter_name=\"x\",\n    perturb_func=gaussian_perturb_func,\n    kernel_func=kernel_func,\n    model_regressor=Ridge(alpha=1),\n    pairwise_distances_metric=\"euclidean\",\n)\n\n(DeviceArray([-5.97378996e-04,  4.81305433e-05,  1.55211031e-03,\n              -3.89112538e-04, -1.21736361e-04, -1.05082065e-04,\n               1.35615395e-04, -4.70238097e-04,  5.06596552e-05,\n               5.96614438e-04,  8.60058644e-04,  1.57593074e-03,\n              -1.35515491e-03,  7.79002556e-04,  8.51082150e-04,\n               1.22845857e-04,  6.43223408e-04,  6.08801842e-04,\n              -3.02861667e-06,  1.18552020e-03], dtype=float32),\n DeviceArray([0.24033982], dtype=float32))\n\n\n\nlime = LimeBase(\n    func=net.apply,\n    additional_func_args={\"params\": params},\n)\nlime.attribute(X)\n\n(DeviceArray([[ 0.00221086,  0.00581249,  0.00237516, ...,  0.0043735 ,\n                0.00298327,  0.00597063],\n              [ 0.00241475,  0.00598198,  0.00254859, ...,  0.00455689,\n                0.00296534,  0.00607178],\n              [-0.00225598, -0.00591312, -0.00256158, ..., -0.00448116,\n               -0.00301292, -0.00606422],\n              ...,\n              [ 0.00223549,  0.00583388,  0.00241553, ...,  0.00438501,\n                0.00298516,  0.00600691],\n              [ 0.00221306,  0.00573398,  0.00225197, ...,  0.00415604,\n                0.00285654,  0.00581799],\n              [ 0.00242912,  0.00596596,  0.00257836, ...,  0.00450296,\n                0.0030849 ,  0.00620169]], dtype=float32),\n DeviceArray([[0.24730496],\n              [0.24756414],\n              [0.23204625],\n              [0.24731565],\n              [0.24734785],\n              [0.24736024],\n              [0.24726894],\n              [0.24740964],\n              [0.24748607],\n              [0.2472707 ],\n              [0.24726589],\n              [0.23272656],\n              [0.2474309 ],\n              [0.24768972],\n              [0.24720442],\n              [0.23258576],\n              [0.24747995],\n              [0.23265731],\n              [0.23216027],\n              [0.23245938],\n              [0.23232539],\n              [0.23272593],\n              [0.24734417],\n              [0.232578  ],\n              [0.2313688 ],\n              [0.232699  ],\n              [0.23166694],\n              [0.2476369 ],\n              [0.24740493],\n              [0.23260298],\n              [0.2476171 ],\n              [0.23259275],\n              [0.2327499 ],\n              [0.24747556],\n              [0.23274088],\n              [0.23278277],\n              [0.23274417],\n              [0.23259158],\n              [0.23276031],\n              [0.23244202],\n              [0.23248667],\n              [0.24736097],\n              [0.2475133 ],\n              [0.23159517],\n              [0.23230171],\n              [0.24782038],\n              [0.23145361],\n              [0.23132221],\n              [0.23273085],\n              [0.24778697],\n              [0.2318807 ],\n              [0.23234296],\n              [0.2473141 ],\n              [0.23186037],\n              [0.24774271],\n              [0.24740867],\n              [0.2321608 ],\n              [0.24721904],\n              [0.24765676],\n              [0.23248613],\n              [0.2325826 ],\n              [0.23253548],\n              [0.24741523],\n              [0.23102273],\n              [0.2324856 ],\n              [0.24752925],\n              [0.24744649],\n              [0.24753611],\n              [0.24741869],\n              [0.24742423],\n              [0.23231731],\n              [0.24737181],\n              [0.23265643],\n              [0.24769214],\n              [0.23244457],\n              [0.23207039],\n              [0.23221391],\n              [0.24714589],\n              [0.24750586],\n              [0.24760035],\n              [0.23261759],\n              [0.23214899],\n              [0.23229751],\n              [0.2317236 ],\n              [0.24728425],\n              [0.24737929],\n              [0.24734789],\n              [0.24735087],\n              [0.24734165],\n              [0.23252094],\n              [0.24735183],\n              [0.23268992],\n              [0.2472564 ],\n              [0.23250158],\n              [0.24733323],\n              [0.23173681],\n              [0.24738492],\n              [0.24749885],\n              [0.23246227],\n              [0.2326699 ],\n              [0.24735057],\n              [0.24739408],\n              [0.23209122],\n              [0.24731648],\n              [0.23272413],\n              [0.232523  ],\n              [0.24749972],\n              [0.24729979],\n              [0.23263235],\n              [0.23183963],\n              [0.23184806],\n              [0.23263596],\n              [0.24731863],\n              [0.23263787],\n              [0.24730252],\n              [0.2472181 ],\n              [0.24739836],\n              [0.24762246],\n              [0.24867377],\n              [0.24735266],\n              [0.23101482],\n              [0.24794899],\n              [0.23271923],\n              [0.23227747],\n              [0.23191595],\n              [0.24739477],\n              [0.23250002],\n              [0.24734001],\n              [0.24730746],\n              [0.24746673],\n              [0.24722606],\n              [0.24733111],\n              [0.23255974],\n              [0.24728492],\n              [0.23227465],\n              [0.24736558],\n              [0.24749716],\n              [0.23218371],\n              [0.23240303],\n              [0.24745671],\n              [0.23237772],\n              [0.24727368],\n              [0.23259522],\n              [0.23260601],\n              [0.23210865],\n              [0.23270965],\n              [0.23201807],\n              [0.2472552 ],\n              [0.2324636 ],\n              [0.24743754],\n              [0.23217565],\n              [0.24725908],\n              [0.23177943],\n              [0.23214754],\n              [0.23217827],\n              [0.24719004],\n              [0.23194984],\n              [0.24750523],\n              [0.23241459],\n              [0.23250034],\n              [0.2324958 ],\n              [0.24732801],\n              [0.24729869],\n              [0.23244049],\n              [0.24737254],\n              [0.23204154],\n              [0.23235755],\n              [0.24782768],\n              [0.24725914],\n              [0.24750242],\n              [0.23217706],\n              [0.23235546],\n              [0.24741676],\n              [0.23215216],\n              [0.24742627],\n              [0.24739274],\n              [0.23202284],\n              [0.23235677],\n              [0.23258376],\n              [0.23233262],\n              [0.2325433 ],\n              [0.2473211 ],\n              [0.23198971],\n              [0.24748974],\n              [0.24733643],\n              [0.23230669],\n              [0.24724506],\n              [0.23218584],\n              [0.24735439],\n              [0.24746102],\n              [0.24732526],\n              [0.2325736 ],\n              [0.23248109],\n              [0.24725024],\n              [0.24732903],\n              [0.24728669],\n              [0.24792492],\n              [0.24741988],\n              [0.23169258],\n              [0.23255627],\n              [0.24753469],\n              [0.24741082],\n              [0.23244117],\n              [0.23260221],\n              [0.2316141 ],\n              [0.24735719],\n              [0.24743626],\n              [0.24751961],\n              [0.23250861],\n              [0.23179816],\n              [0.24762237],\n              [0.24788071],\n              [0.2325231 ],\n              [0.2324555 ],\n              [0.2325428 ],\n              [0.23237003],\n              [0.2323332 ],\n              [0.24787727],\n              [0.23265064],\n              [0.24732669],\n              [0.24749607],\n              [0.24766587],\n              [0.2480405 ],\n              [0.23218127],\n              [0.24730903],\n              [0.23179471],\n              [0.2473682 ],\n              [0.24743044],\n              [0.247412  ],\n              [0.23198949],\n              [0.2474296 ],\n              [0.23019618],\n              [0.24736585],\n              [0.24736209],\n              [0.24728309],\n              [0.24746317],\n              [0.23231001],\n              [0.23207894],\n              [0.24735352],\n              [0.2322949 ],\n              [0.24727553],\n              [0.24729888],\n              [0.23229258],\n              [0.23158833],\n              [0.24727146],\n              [0.23261805],\n              [0.24736366],\n              [0.23235823],\n              [0.24729802],\n              [0.24752894],\n              [0.24734056],\n              [0.23284838],\n              [0.24715085],\n              [0.2323496 ],\n              [0.24730666],\n              [0.2320316 ],\n              [0.23239292],\n              [0.24736963],\n              [0.24747422],\n              [0.24727249],\n              [0.23237279],\n              [0.24724723],\n              [0.24729113],\n              [0.24737135],\n              [0.24733087],\n              [0.24724013],\n              [0.24737415],\n              [0.2327153 ],\n              [0.24791467],\n              [0.24732663],\n              [0.24748352],\n              [0.23251979],\n              [0.24714312],\n              [0.24755022],\n              [0.2473973 ],\n              [0.23273005],\n              [0.23246035],\n              [0.24732141],\n              [0.23213631],\n              [0.24721566],\n              [0.23201378],\n              [0.23221391],\n              [0.24740039],\n              [0.23237988],\n              [0.23115295],\n              [0.23251712],\n              [0.23238027],\n              [0.23267289],\n              [0.24774548],\n              [0.23251162],\n              [0.23228587],\n              [0.23244764],\n              [0.24746987],\n              [0.23209953],\n              [0.24765822],\n              [0.23273446],\n              [0.23225085],\n              [0.24744767],\n              [0.23257345],\n              [0.24717891],\n              [0.23226202],\n              [0.23245876],\n              [0.24770094],\n              [0.23234405],\n              [0.24729179],\n              [0.23253849],\n              [0.23176861],\n              [0.23238832],\n              [0.24772736],\n              [0.23276523],\n              [0.24727891],\n              [0.24722643],\n              [0.23272316],\n              [0.24743803],\n              [0.2473214 ],\n              [0.2324927 ],\n              [0.24720925],\n              [0.2319577 ],\n              [0.24736662],\n              [0.24720564],\n              [0.2473594 ],\n              [0.24746218],\n              [0.2473249 ],\n              [0.23269431],\n              [0.23267029],\n              [0.24746528],\n              [0.24735796],\n              [0.23275968],\n              [0.24729164],\n              [0.24727115],\n              [0.24726492],\n              [0.23244949],\n              [0.23239763],\n              [0.23260587],\n              [0.2317266 ],\n              [0.24721311],\n              [0.24791732],\n              [0.23277605],\n              [0.24746867],\n              [0.2326223 ],\n              [0.24716686],\n              [0.23254141],\n              [0.24743882],\n              [0.24725032],\n              [0.23271337],\n              [0.23271807],\n              [0.24727352],\n              [0.23150992],\n              [0.24748631],\n              [0.24734168],\n              [0.24780883],\n              [0.24741678],\n              [0.24733034],\n              [0.23234318],\n              [0.24731077],\n              [0.2472419 ],\n              [0.2473333 ],\n              [0.23262657],\n              [0.24725671],\n              [0.24735816],\n              [0.23228347],\n              [0.2473127 ],\n              [0.24725196],\n              [0.23249117],\n              [0.23216985],\n              [0.24734968],\n              [0.23251817],\n              [0.23218517],\n              [0.23229903],\n              [0.24736002],\n              [0.23227802],\n              [0.24734715],\n              [0.24728952],\n              [0.23249188],\n              [0.23238972],\n              [0.24732132],\n              [0.23275092],\n              [0.23262392],\n              [0.23213783],\n              [0.24741286],\n              [0.24725187],\n              [0.23273003],\n              [0.23257877],\n              [0.24740161],\n              [0.24715275],\n              [0.24721095],\n              [0.23270902],\n              [0.24722742],\n              [0.2319201 ],\n              [0.23252024],\n              [0.23254202],\n              [0.23239242],\n              [0.23167515],\n              [0.24744089],\n              [0.23231809],\n              [0.23276606],\n              [0.23184034],\n              [0.23192035],\n              [0.23264523],\n              [0.24742319],\n              [0.23254554],\n              [0.23217438],\n              [0.24735396],\n              [0.23255692],\n              [0.23227966],\n              [0.232184  ],\n              [0.24756455],\n              [0.23222198],\n              [0.24758385],\n              [0.24754198],\n              [0.24729684],\n              [0.23244324],\n              [0.24749175],\n              [0.23269522],\n              [0.23192582],\n              [0.2326539 ],\n              [0.23266825],\n              [0.24736997],\n              [0.24730414],\n              [0.2324066 ],\n              [0.24725583],\n              [0.24741372],\n              [0.24747895],\n              [0.23201811],\n              [0.24729444],\n              [0.247381  ],\n              [0.2326003 ],\n              [0.23236334],\n              [0.24736139],\n              [0.2321806 ],\n              [0.23245579],\n              [0.23235963],\n              [0.23238862],\n              [0.2472786 ],\n              [0.23169187],\n              [0.24740751],\n              [0.23263946],\n              [0.23267938],\n              [0.24756868],\n              [0.23228672],\n              [0.24769364],\n              [0.24736963],\n              [0.23052931],\n              [0.23218985],\n              [0.23187762],\n              [0.23254694],\n              [0.23233788],\n              [0.2312679 ],\n              [0.23229599],\n              [0.23235682],\n              [0.23257495],\n              [0.23236491],\n              [0.24776962],\n              [0.24718513],\n              [0.24725026],\n              [0.23261815],\n              [0.2471972 ],\n              [0.24723697],\n              [0.2323229 ],\n              [0.23140587],\n              [0.2477279 ],\n              [0.23209357],\n              [0.24751735],\n              [0.24730821],\n              [0.24750674],\n              [0.23272803],\n              [0.24727444],\n              [0.24727756],\n              [0.24726333],\n              [0.24735498],\n              [0.23130915],\n              [0.24735644],\n              [0.2326549 ],\n              [0.23219867],\n              [0.23273338],\n              [0.23268846],\n              [0.24724345],\n              [0.2473797 ],\n              [0.24738654],\n              [0.2314789 ],\n              [0.24723507],\n              [0.23221129],\n              [0.24730465],\n              [0.23243599],\n              [0.232267  ],\n              [0.23234174],\n              [0.24730346],\n              [0.2472916 ],\n              [0.24730274],\n              [0.2325149 ],\n              [0.24734567],\n              [0.24727803],\n              [0.24726497],\n              [0.23281087],\n              [0.24736117],\n              [0.23239669],\n              [0.23156339],\n              [0.2475236 ],\n              [0.24708891],\n              [0.2476187 ]], dtype=float32))",
    "crumbs": [
      "API",
      "LIME"
    ]
  }
]
{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp attr.lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from ipynb_path import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from explainax.imports import *\n",
    "from explainax.attr.base import *\n",
    "from explainax.linear_model import Lasso, Ridge, LinearModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@partial(jit, static_argnums=(2))\n",
    "def pairwise_distances(\n",
    "    x: Array, # [n, k]\n",
    "    y: Array, # [m, k]\n",
    "    metric: str = \"euclidean\" # Supports \"euclidean\" and \"cosine\"\n",
    ") -> Array: # [n, m]\n",
    "    def euclidean_distances(x: Array, y: Array) -> float:\n",
    "        XX = jnp.dot(x, x)\n",
    "        YY = jnp.dot(y, y)\n",
    "        XY = jnp.dot(x, y)\n",
    "        dist = jnp.clip(XX - 2 * XY + YY, a_min=0.)\n",
    "        return jnp.sqrt(dist)\n",
    "        # return jnp.linalg.norm(x - y, ord=2)\n",
    "    \n",
    "    def cosine_distances(x: Array, y: Array) -> float:\n",
    "        return 1.0 - jnp.dot(x, y) / (jnp.linalg.norm(x) * jnp.linalg.norm(y) + 1e-8)\n",
    "    \n",
    "    if metric == \"euclidean\":\n",
    "        dists_fn = vmap(vmap(euclidean_distances, in_axes=(None, 0)), in_axes=(0, None))\n",
    "    elif metric == \"cosine\":\n",
    "        dists_fn = vmap(vmap(cosine_distances, in_axes=(None, 0)), in_axes=(0, None))\n",
    "    else:\n",
    "        raise ValueError(f\"metric='{metric}' not supported\")\n",
    "    \n",
    "    return dists_fn(x, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is similar to \n",
    "[sklearn.metrics.pairwise_distances](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances as sk_pairwise_distances"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pairwise_distances` is faster than sklearn's implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[euclidean] Sklearn pairwise_distances:\n",
      "28.6 ms ± 6.82 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "[euclidean] JAX pairwise_distances:\n",
      "6.27 ms ± 3.02 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "[cosine] Sklearn pairwise_distances:\n",
      "29.2 ms ± 6.07 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "[cosine] JAX pairwise_distances:\n",
      "6.4 ms ± 2.82 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "X = np.random.normal(size=(1000, 28 * 28))\n",
    "Y = np.random.normal(size=(1000, 28 * 28))\n",
    "\n",
    "def benchmark_pairwise_distances(metric):\n",
    "    print(f\"[{metric}] Sklearn pairwise_distances:\")\n",
    "    %timeit -n 10 sk_pairwise_distances(X, Y, metric=metric)\n",
    "    print(f\"[{metric}] JAX pairwise_distances:\")\n",
    "    %timeit -n 10 pairwise_distances(X, Y, metric=metric).block_until_ready()\n",
    "    assert jnp.allclose(\n",
    "        sk_pairwise_distances(X, Y, metric=metric),\n",
    "        pairwise_distances(X, Y, metric=metric)\n",
    "    )\n",
    "\n",
    "benchmark_pairwise_distances(\"euclidean\")\n",
    "benchmark_pairwise_distances(\"cosine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def bernoulli_perturb_func(x: Array, prng_key: jrand.PRNGKey, **kwargs) -> Array:\n",
    "    \"\"\"Bernoulli perturbation function for LIME\"\"\"\n",
    "    probs = jnp.ones(x.shape) * 0.5\n",
    "    return jrand.bernoulli(prng_key, p=probs, shape=x.shape)\n",
    "\n",
    "def gaussian_perturb_func(x: Array, prng_key: jrand.PRNGKey, **kwargs) -> Array:\n",
    "    \"\"\"Gaussian perturbation function for LIME\"\"\"\n",
    "    return jrand.normal(prng_key, shape=x.shape) #+ x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _perturb_data(\n",
    "    x: Array, # [1, k]\n",
    "    n_samples: int,\n",
    "    perturb_func: Callable[[Array, jrand.PRNGKey], Array],\n",
    "    prng_key: jrand.PRNGKey,\n",
    ") -> Array:\n",
    "    \"\"\"Perturb data using perturb_func\"\"\"\n",
    "    perturbed_data = vmap(jit(perturb_func))(\n",
    "        jnp.repeat(x, n_samples, axis=0), \n",
    "        jrand.split(prng_key, n_samples)\n",
    "    ) \n",
    "    return jnp.concatenate([x, perturbed_data], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.normal(size=(1, 28 * 28))\n",
    "b_perturbed = _perturb_data(X, 100, bernoulli_perturb_func, jrand.PRNGKey(42))\n",
    "g_perturbed = _perturb_data(X, 100, gaussian_perturb_func, jrand.PRNGKey(42))\n",
    "assert b_perturbed.shape == (101, 28 * 28)\n",
    "assert g_perturbed.shape == (101, 28 * 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@jit\n",
    "def exp_kernel_func(dists: Array, kernel_width: float) -> Array:\n",
    "    \"\"\"Exponential kernel function for LIME\"\"\"\n",
    "    return jnp.sqrt(jnp.exp(-(dists ** 2) / kernel_width ** 2) + 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 389.12 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "273 µs ± 657 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "distances = pairwise_distances(g_perturbed, X)\n",
    "%timeit -n 10 exp_kernel_func(distances, 0.75 * 28 * 28).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _lime_attribute_single_instance(\n",
    "    inputs: Array, # [k]\n",
    "    n_samples: int,\n",
    "    rng_key: jrand.PRNGKey,\n",
    "    bb_func: Callable[[Array], Array],\n",
    "    additional_func_args: Dict,\n",
    "    input_paramter_name: str,\n",
    "    perturb_func: Callable[[Array, jrand.PRNGKey, Any], Array],\n",
    "    kernel_func: Callable[[Array], Array],\n",
    "    model_regressor: LinearModel,\n",
    "    pairwise_distances_metric: str,\n",
    " ) -> Tuple[Array, Array]: # (Local explanation, intercept)\n",
    "    # Perturb data\n",
    "    inputs = inputs.reshape(1, -1)\n",
    "    data = _perturb_data(inputs, n_samples, perturb_func, rng_key)\n",
    "    distances = pairwise_distances(data, inputs, metric=pairwise_distances_metric).ravel()\n",
    "    yss = bb_func(**{input_paramter_name: data}, **additional_func_args)\n",
    "\n",
    "    if len(yss.shape) == 1:\n",
    "        yss = yss.reshape(-1, 1)\n",
    "    if yss.shape != (n_samples + 1, 1):\n",
    "        raise ValueError(\"Black-box function must output a single value for each instance.\")\n",
    "    \n",
    "    # fit a linear model to the perturbed data\n",
    "    # TODO: implement feature selection\n",
    "    # https://github.com/marcotcr/lime/blob/fd7eb2e6f760619c29fca0187c07b82157601b32/lime/lime_base.py#L70\n",
    "    weights = kernel_func(distances)\n",
    "    assert data.shape == (n_samples + 1, inputs.shape[1])\n",
    "    assert yss.shape == (n_samples + 1, 1)\n",
    "    assert weights.shape == (n_samples + 1,)\n",
    "    model_regressor.fit(data, yss, weights=weights)\n",
    "    return (model_regressor.coef_, model_regressor.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class LimeBase(Attribution):\n",
    "    def __init__(\n",
    "        self,\n",
    "        func: Callable, # A black-box function to be explained\n",
    "        additional_func_args: Dict = None, # Additional arguments for the black-box function\n",
    "        model_regressor = None, # Linear regressor to use in explanation\n",
    "        kernal_func: Callable = None, # Kernel function for computing similarity\n",
    "        kernel_width: float = None, # Kernel width for computing similarity. Defaults to (n_features * 0.75)\n",
    "        perturb_func: Callable = None, # Perturbation function for generating perturbed instances\n",
    "        input_paramter_name: str = \"x\", # Name of the input parameter for the black-box function\n",
    "        pairwise_distances_metric: str = \"euclidean\", # Metric for computing pairwise distances\n",
    "    ): \n",
    "        super().__init__(func, additional_func_args)\n",
    "        self.bb_func = func\n",
    "        self.model_regressor = model_regressor if model_regressor is not None else Ridge(alpha=1)\n",
    "        self.kernal_func = kernal_func if kernal_func is not None else exp_kernel_func\n",
    "        self.kernel_width = kernel_width\n",
    "        self.perturb_func = perturb_func if perturb_func is not None else bernoulli_perturb_func\n",
    "        self.x_name = input_paramter_name\n",
    "        self.metric = pairwise_distances_metric\n",
    "\n",
    "    def attribute(\n",
    "        self, \n",
    "        inputs: Array, \n",
    "        n_samples: int = 100,\n",
    "        rng_key: jrand.PRNGKey = None,\n",
    "        **kwargs\n",
    "    ) -> Array:\n",
    "        \"\"\"Compute attribution for a given input\"\"\"\n",
    "        if len(inputs.shape) != 2:\n",
    "            raise ValueError(\"Inputs shape must be (n_instances, n_features).\")\n",
    "        if rng_key is None:\n",
    "            rng_key = jrand.PRNGKey(get_config().global_seed)\n",
    "\n",
    "        kernel_width = self.kernel_width if self.kernel_width is not None else inputs.shape[-1] * 0.75\n",
    "        kerenl_func = partial(self.kernal_func, kernel_width=kernel_width)\n",
    "        perturb_func = partial(self.perturb_func, **kwargs)\n",
    "        additional_func_args = self.additional_func_args if self.additional_func_args is not None else {}\n",
    "        partialed_lime_func = partial(\n",
    "            _lime_attribute_single_instance,\n",
    "            n_samples=n_samples,\n",
    "            rng_key=rng_key,\n",
    "            bb_func=self.bb_func,\n",
    "            additional_func_args=additional_func_args,\n",
    "            input_paramter_name=self.x_name,\n",
    "            perturb_func=perturb_func,\n",
    "            kernel_func=kerenl_func,\n",
    "            model_regressor=self.model_regressor,\n",
    "            pairwise_distances_metric=self.metric,\n",
    "        )\n",
    "\n",
    "        exp, intercept = vmap(partialed_lime_func)(inputs)\n",
    "        return (exp, intercept)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "import haiku as hk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys = make_regression(n_samples=500, n_features=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = LinearModel()\n",
    "linear_model.fit(xs, ys)\n",
    "lime = LimeBase(linear_model.predict, input_paramter_name=\"X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a simple haiku model\n",
    "def model(x):\n",
    "    mlp = hk.Sequential([\n",
    "        hk.Linear(10),\n",
    "        jax.nn.relu,\n",
    "        hk.Linear(10),\n",
    "        jax.nn.relu,\n",
    "        hk.Linear(1),\n",
    "    ])\n",
    "    return mlp(x)\n",
    "\n",
    "\n",
    "def init(x):\n",
    "    net = hk.without_apply_rng(hk.transform(model))\n",
    "    opt = optax.sgd(1e-1)\n",
    "    params = net.init(jrand.PRNGKey(42), x)\n",
    "    opt_state = opt.init(params)\n",
    "    return net, opt, params, opt_state\n",
    "\n",
    "def loss(params, net, x, y):\n",
    "    pred = net.apply(params, x)\n",
    "    return jnp.mean((pred - y) ** 2)\n",
    "\n",
    "@partial(jax.jit, static_argnums=(2,3))\n",
    "def update(\n",
    "    params: hk.Params,\n",
    "    opt_state: optax.OptState,\n",
    "    net: hk.Transformed,\n",
    "    opt: optax.GradientTransformation,\n",
    "    x: jnp.ndarray,\n",
    "    y: jnp.ndarray\n",
    "):\n",
    "    grads = jax.grad(loss)(params, net, x, y)\n",
    "    updates, opt_state = opt.update(grads, opt_state)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, opt_state\n",
    "\n",
    "def train(\n",
    "    net: hk.Transformed,\n",
    "    opt: optax.GradientTransformation,\n",
    "    params: hk.Params,\n",
    "    opt_state: optax.OptState,\n",
    "    x: jnp.ndarray,\n",
    "    y: jnp.ndarray,\n",
    "    n_epochs: int = 100,\n",
    "    batch_size: int = 32,\n",
    "):\n",
    "    n_samples = x.shape[0]\n",
    "    for _ in range(n_epochs):\n",
    "        for i in range(0, n_samples, batch_size):\n",
    "            x_batch = x[i:i+batch_size]\n",
    "            y_batch = y[i:i+batch_size]\n",
    "            params, opt_state = update(params, opt_state, net, opt, x_batch, y_batch)\n",
    "    return params\n",
    "\n",
    "def fit_a_model(\n",
    "    X: Array,\n",
    "    y: Array,\n",
    "):\n",
    "    net, opt, params, opt_state = init(X)\n",
    "    params = train(net, opt, params, opt_state, X, y)\n",
    "    return net, params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/birk/mambaforge-pypy3/envs/nbdev2/lib/python3.7/site-packages/haiku/_src/base.py:515: UserWarning: Explicitly requested dtype float64 requested in zeros is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  param = init(shape, dtype)\n"
     ]
    }
   ],
   "source": [
    "net, params = fit_a_model(xs, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[0.24004635],\n",
       "             [0.24004635],\n",
       "             [0.24004635],\n",
       "             [0.24004635],\n",
       "             [0.24004635],\n",
       "             [0.24004635],\n",
       "             [0.24004635],\n",
       "             [0.24004635],\n",
       "             [0.24004635],\n",
       "             [0.24004635]], dtype=float32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.apply(params, xs[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DeviceArray([-5.97378996e-04,  4.81305433e-05,  1.55211031e-03,\n",
       "              -3.89112538e-04, -1.21736361e-04, -1.05082065e-04,\n",
       "               1.35615395e-04, -4.70238097e-04,  5.06596552e-05,\n",
       "               5.96614438e-04,  8.60058644e-04,  1.57593074e-03,\n",
       "              -1.35515491e-03,  7.79002556e-04,  8.51082150e-04,\n",
       "               1.22845857e-04,  6.43223408e-04,  6.08801842e-04,\n",
       "              -3.02861667e-06,  1.18552020e-03], dtype=float32),\n",
       " DeviceArray([0.24033982], dtype=float32))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel_func = partial(exp_kernel_func, kernel_width=2 * 0.75)\n",
    "\n",
    "_lime_attribute_single_instance(\n",
    "    X[:1],\n",
    "    1000,\n",
    "    jrand.PRNGKey(42),\n",
    "    net.apply,\n",
    "    additional_func_args={\"params\": params},\n",
    "    input_paramter_name=\"x\",\n",
    "    perturb_func=gaussian_perturb_func,\n",
    "    kernel_func=kernel_func,\n",
    "    model_regressor=Ridge(alpha=1),\n",
    "    pairwise_distances_metric=\"euclidean\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DeviceArray([[ 0.00221086,  0.00581249,  0.00237516, ...,  0.0043735 ,\n",
       "                0.00298327,  0.00597063],\n",
       "              [ 0.00241475,  0.00598198,  0.00254859, ...,  0.00455689,\n",
       "                0.00296534,  0.00607178],\n",
       "              [-0.00225598, -0.00591312, -0.00256158, ..., -0.00448116,\n",
       "               -0.00301292, -0.00606422],\n",
       "              ...,\n",
       "              [ 0.00223549,  0.00583388,  0.00241553, ...,  0.00438501,\n",
       "                0.00298516,  0.00600691],\n",
       "              [ 0.00221306,  0.00573398,  0.00225197, ...,  0.00415604,\n",
       "                0.00285654,  0.00581799],\n",
       "              [ 0.00242912,  0.00596596,  0.00257836, ...,  0.00450296,\n",
       "                0.0030849 ,  0.00620169]], dtype=float32),\n",
       " DeviceArray([[0.24730496],\n",
       "              [0.24756414],\n",
       "              [0.23204625],\n",
       "              [0.24731565],\n",
       "              [0.24734785],\n",
       "              [0.24736024],\n",
       "              [0.24726894],\n",
       "              [0.24740964],\n",
       "              [0.24748607],\n",
       "              [0.2472707 ],\n",
       "              [0.24726589],\n",
       "              [0.23272656],\n",
       "              [0.2474309 ],\n",
       "              [0.24768972],\n",
       "              [0.24720442],\n",
       "              [0.23258576],\n",
       "              [0.24747995],\n",
       "              [0.23265731],\n",
       "              [0.23216027],\n",
       "              [0.23245938],\n",
       "              [0.23232539],\n",
       "              [0.23272593],\n",
       "              [0.24734417],\n",
       "              [0.232578  ],\n",
       "              [0.2313688 ],\n",
       "              [0.232699  ],\n",
       "              [0.23166694],\n",
       "              [0.2476369 ],\n",
       "              [0.24740493],\n",
       "              [0.23260298],\n",
       "              [0.2476171 ],\n",
       "              [0.23259275],\n",
       "              [0.2327499 ],\n",
       "              [0.24747556],\n",
       "              [0.23274088],\n",
       "              [0.23278277],\n",
       "              [0.23274417],\n",
       "              [0.23259158],\n",
       "              [0.23276031],\n",
       "              [0.23244202],\n",
       "              [0.23248667],\n",
       "              [0.24736097],\n",
       "              [0.2475133 ],\n",
       "              [0.23159517],\n",
       "              [0.23230171],\n",
       "              [0.24782038],\n",
       "              [0.23145361],\n",
       "              [0.23132221],\n",
       "              [0.23273085],\n",
       "              [0.24778697],\n",
       "              [0.2318807 ],\n",
       "              [0.23234296],\n",
       "              [0.2473141 ],\n",
       "              [0.23186037],\n",
       "              [0.24774271],\n",
       "              [0.24740867],\n",
       "              [0.2321608 ],\n",
       "              [0.24721904],\n",
       "              [0.24765676],\n",
       "              [0.23248613],\n",
       "              [0.2325826 ],\n",
       "              [0.23253548],\n",
       "              [0.24741523],\n",
       "              [0.23102273],\n",
       "              [0.2324856 ],\n",
       "              [0.24752925],\n",
       "              [0.24744649],\n",
       "              [0.24753611],\n",
       "              [0.24741869],\n",
       "              [0.24742423],\n",
       "              [0.23231731],\n",
       "              [0.24737181],\n",
       "              [0.23265643],\n",
       "              [0.24769214],\n",
       "              [0.23244457],\n",
       "              [0.23207039],\n",
       "              [0.23221391],\n",
       "              [0.24714589],\n",
       "              [0.24750586],\n",
       "              [0.24760035],\n",
       "              [0.23261759],\n",
       "              [0.23214899],\n",
       "              [0.23229751],\n",
       "              [0.2317236 ],\n",
       "              [0.24728425],\n",
       "              [0.24737929],\n",
       "              [0.24734789],\n",
       "              [0.24735087],\n",
       "              [0.24734165],\n",
       "              [0.23252094],\n",
       "              [0.24735183],\n",
       "              [0.23268992],\n",
       "              [0.2472564 ],\n",
       "              [0.23250158],\n",
       "              [0.24733323],\n",
       "              [0.23173681],\n",
       "              [0.24738492],\n",
       "              [0.24749885],\n",
       "              [0.23246227],\n",
       "              [0.2326699 ],\n",
       "              [0.24735057],\n",
       "              [0.24739408],\n",
       "              [0.23209122],\n",
       "              [0.24731648],\n",
       "              [0.23272413],\n",
       "              [0.232523  ],\n",
       "              [0.24749972],\n",
       "              [0.24729979],\n",
       "              [0.23263235],\n",
       "              [0.23183963],\n",
       "              [0.23184806],\n",
       "              [0.23263596],\n",
       "              [0.24731863],\n",
       "              [0.23263787],\n",
       "              [0.24730252],\n",
       "              [0.2472181 ],\n",
       "              [0.24739836],\n",
       "              [0.24762246],\n",
       "              [0.24867377],\n",
       "              [0.24735266],\n",
       "              [0.23101482],\n",
       "              [0.24794899],\n",
       "              [0.23271923],\n",
       "              [0.23227747],\n",
       "              [0.23191595],\n",
       "              [0.24739477],\n",
       "              [0.23250002],\n",
       "              [0.24734001],\n",
       "              [0.24730746],\n",
       "              [0.24746673],\n",
       "              [0.24722606],\n",
       "              [0.24733111],\n",
       "              [0.23255974],\n",
       "              [0.24728492],\n",
       "              [0.23227465],\n",
       "              [0.24736558],\n",
       "              [0.24749716],\n",
       "              [0.23218371],\n",
       "              [0.23240303],\n",
       "              [0.24745671],\n",
       "              [0.23237772],\n",
       "              [0.24727368],\n",
       "              [0.23259522],\n",
       "              [0.23260601],\n",
       "              [0.23210865],\n",
       "              [0.23270965],\n",
       "              [0.23201807],\n",
       "              [0.2472552 ],\n",
       "              [0.2324636 ],\n",
       "              [0.24743754],\n",
       "              [0.23217565],\n",
       "              [0.24725908],\n",
       "              [0.23177943],\n",
       "              [0.23214754],\n",
       "              [0.23217827],\n",
       "              [0.24719004],\n",
       "              [0.23194984],\n",
       "              [0.24750523],\n",
       "              [0.23241459],\n",
       "              [0.23250034],\n",
       "              [0.2324958 ],\n",
       "              [0.24732801],\n",
       "              [0.24729869],\n",
       "              [0.23244049],\n",
       "              [0.24737254],\n",
       "              [0.23204154],\n",
       "              [0.23235755],\n",
       "              [0.24782768],\n",
       "              [0.24725914],\n",
       "              [0.24750242],\n",
       "              [0.23217706],\n",
       "              [0.23235546],\n",
       "              [0.24741676],\n",
       "              [0.23215216],\n",
       "              [0.24742627],\n",
       "              [0.24739274],\n",
       "              [0.23202284],\n",
       "              [0.23235677],\n",
       "              [0.23258376],\n",
       "              [0.23233262],\n",
       "              [0.2325433 ],\n",
       "              [0.2473211 ],\n",
       "              [0.23198971],\n",
       "              [0.24748974],\n",
       "              [0.24733643],\n",
       "              [0.23230669],\n",
       "              [0.24724506],\n",
       "              [0.23218584],\n",
       "              [0.24735439],\n",
       "              [0.24746102],\n",
       "              [0.24732526],\n",
       "              [0.2325736 ],\n",
       "              [0.23248109],\n",
       "              [0.24725024],\n",
       "              [0.24732903],\n",
       "              [0.24728669],\n",
       "              [0.24792492],\n",
       "              [0.24741988],\n",
       "              [0.23169258],\n",
       "              [0.23255627],\n",
       "              [0.24753469],\n",
       "              [0.24741082],\n",
       "              [0.23244117],\n",
       "              [0.23260221],\n",
       "              [0.2316141 ],\n",
       "              [0.24735719],\n",
       "              [0.24743626],\n",
       "              [0.24751961],\n",
       "              [0.23250861],\n",
       "              [0.23179816],\n",
       "              [0.24762237],\n",
       "              [0.24788071],\n",
       "              [0.2325231 ],\n",
       "              [0.2324555 ],\n",
       "              [0.2325428 ],\n",
       "              [0.23237003],\n",
       "              [0.2323332 ],\n",
       "              [0.24787727],\n",
       "              [0.23265064],\n",
       "              [0.24732669],\n",
       "              [0.24749607],\n",
       "              [0.24766587],\n",
       "              [0.2480405 ],\n",
       "              [0.23218127],\n",
       "              [0.24730903],\n",
       "              [0.23179471],\n",
       "              [0.2473682 ],\n",
       "              [0.24743044],\n",
       "              [0.247412  ],\n",
       "              [0.23198949],\n",
       "              [0.2474296 ],\n",
       "              [0.23019618],\n",
       "              [0.24736585],\n",
       "              [0.24736209],\n",
       "              [0.24728309],\n",
       "              [0.24746317],\n",
       "              [0.23231001],\n",
       "              [0.23207894],\n",
       "              [0.24735352],\n",
       "              [0.2322949 ],\n",
       "              [0.24727553],\n",
       "              [0.24729888],\n",
       "              [0.23229258],\n",
       "              [0.23158833],\n",
       "              [0.24727146],\n",
       "              [0.23261805],\n",
       "              [0.24736366],\n",
       "              [0.23235823],\n",
       "              [0.24729802],\n",
       "              [0.24752894],\n",
       "              [0.24734056],\n",
       "              [0.23284838],\n",
       "              [0.24715085],\n",
       "              [0.2323496 ],\n",
       "              [0.24730666],\n",
       "              [0.2320316 ],\n",
       "              [0.23239292],\n",
       "              [0.24736963],\n",
       "              [0.24747422],\n",
       "              [0.24727249],\n",
       "              [0.23237279],\n",
       "              [0.24724723],\n",
       "              [0.24729113],\n",
       "              [0.24737135],\n",
       "              [0.24733087],\n",
       "              [0.24724013],\n",
       "              [0.24737415],\n",
       "              [0.2327153 ],\n",
       "              [0.24791467],\n",
       "              [0.24732663],\n",
       "              [0.24748352],\n",
       "              [0.23251979],\n",
       "              [0.24714312],\n",
       "              [0.24755022],\n",
       "              [0.2473973 ],\n",
       "              [0.23273005],\n",
       "              [0.23246035],\n",
       "              [0.24732141],\n",
       "              [0.23213631],\n",
       "              [0.24721566],\n",
       "              [0.23201378],\n",
       "              [0.23221391],\n",
       "              [0.24740039],\n",
       "              [0.23237988],\n",
       "              [0.23115295],\n",
       "              [0.23251712],\n",
       "              [0.23238027],\n",
       "              [0.23267289],\n",
       "              [0.24774548],\n",
       "              [0.23251162],\n",
       "              [0.23228587],\n",
       "              [0.23244764],\n",
       "              [0.24746987],\n",
       "              [0.23209953],\n",
       "              [0.24765822],\n",
       "              [0.23273446],\n",
       "              [0.23225085],\n",
       "              [0.24744767],\n",
       "              [0.23257345],\n",
       "              [0.24717891],\n",
       "              [0.23226202],\n",
       "              [0.23245876],\n",
       "              [0.24770094],\n",
       "              [0.23234405],\n",
       "              [0.24729179],\n",
       "              [0.23253849],\n",
       "              [0.23176861],\n",
       "              [0.23238832],\n",
       "              [0.24772736],\n",
       "              [0.23276523],\n",
       "              [0.24727891],\n",
       "              [0.24722643],\n",
       "              [0.23272316],\n",
       "              [0.24743803],\n",
       "              [0.2473214 ],\n",
       "              [0.2324927 ],\n",
       "              [0.24720925],\n",
       "              [0.2319577 ],\n",
       "              [0.24736662],\n",
       "              [0.24720564],\n",
       "              [0.2473594 ],\n",
       "              [0.24746218],\n",
       "              [0.2473249 ],\n",
       "              [0.23269431],\n",
       "              [0.23267029],\n",
       "              [0.24746528],\n",
       "              [0.24735796],\n",
       "              [0.23275968],\n",
       "              [0.24729164],\n",
       "              [0.24727115],\n",
       "              [0.24726492],\n",
       "              [0.23244949],\n",
       "              [0.23239763],\n",
       "              [0.23260587],\n",
       "              [0.2317266 ],\n",
       "              [0.24721311],\n",
       "              [0.24791732],\n",
       "              [0.23277605],\n",
       "              [0.24746867],\n",
       "              [0.2326223 ],\n",
       "              [0.24716686],\n",
       "              [0.23254141],\n",
       "              [0.24743882],\n",
       "              [0.24725032],\n",
       "              [0.23271337],\n",
       "              [0.23271807],\n",
       "              [0.24727352],\n",
       "              [0.23150992],\n",
       "              [0.24748631],\n",
       "              [0.24734168],\n",
       "              [0.24780883],\n",
       "              [0.24741678],\n",
       "              [0.24733034],\n",
       "              [0.23234318],\n",
       "              [0.24731077],\n",
       "              [0.2472419 ],\n",
       "              [0.2473333 ],\n",
       "              [0.23262657],\n",
       "              [0.24725671],\n",
       "              [0.24735816],\n",
       "              [0.23228347],\n",
       "              [0.2473127 ],\n",
       "              [0.24725196],\n",
       "              [0.23249117],\n",
       "              [0.23216985],\n",
       "              [0.24734968],\n",
       "              [0.23251817],\n",
       "              [0.23218517],\n",
       "              [0.23229903],\n",
       "              [0.24736002],\n",
       "              [0.23227802],\n",
       "              [0.24734715],\n",
       "              [0.24728952],\n",
       "              [0.23249188],\n",
       "              [0.23238972],\n",
       "              [0.24732132],\n",
       "              [0.23275092],\n",
       "              [0.23262392],\n",
       "              [0.23213783],\n",
       "              [0.24741286],\n",
       "              [0.24725187],\n",
       "              [0.23273003],\n",
       "              [0.23257877],\n",
       "              [0.24740161],\n",
       "              [0.24715275],\n",
       "              [0.24721095],\n",
       "              [0.23270902],\n",
       "              [0.24722742],\n",
       "              [0.2319201 ],\n",
       "              [0.23252024],\n",
       "              [0.23254202],\n",
       "              [0.23239242],\n",
       "              [0.23167515],\n",
       "              [0.24744089],\n",
       "              [0.23231809],\n",
       "              [0.23276606],\n",
       "              [0.23184034],\n",
       "              [0.23192035],\n",
       "              [0.23264523],\n",
       "              [0.24742319],\n",
       "              [0.23254554],\n",
       "              [0.23217438],\n",
       "              [0.24735396],\n",
       "              [0.23255692],\n",
       "              [0.23227966],\n",
       "              [0.232184  ],\n",
       "              [0.24756455],\n",
       "              [0.23222198],\n",
       "              [0.24758385],\n",
       "              [0.24754198],\n",
       "              [0.24729684],\n",
       "              [0.23244324],\n",
       "              [0.24749175],\n",
       "              [0.23269522],\n",
       "              [0.23192582],\n",
       "              [0.2326539 ],\n",
       "              [0.23266825],\n",
       "              [0.24736997],\n",
       "              [0.24730414],\n",
       "              [0.2324066 ],\n",
       "              [0.24725583],\n",
       "              [0.24741372],\n",
       "              [0.24747895],\n",
       "              [0.23201811],\n",
       "              [0.24729444],\n",
       "              [0.247381  ],\n",
       "              [0.2326003 ],\n",
       "              [0.23236334],\n",
       "              [0.24736139],\n",
       "              [0.2321806 ],\n",
       "              [0.23245579],\n",
       "              [0.23235963],\n",
       "              [0.23238862],\n",
       "              [0.2472786 ],\n",
       "              [0.23169187],\n",
       "              [0.24740751],\n",
       "              [0.23263946],\n",
       "              [0.23267938],\n",
       "              [0.24756868],\n",
       "              [0.23228672],\n",
       "              [0.24769364],\n",
       "              [0.24736963],\n",
       "              [0.23052931],\n",
       "              [0.23218985],\n",
       "              [0.23187762],\n",
       "              [0.23254694],\n",
       "              [0.23233788],\n",
       "              [0.2312679 ],\n",
       "              [0.23229599],\n",
       "              [0.23235682],\n",
       "              [0.23257495],\n",
       "              [0.23236491],\n",
       "              [0.24776962],\n",
       "              [0.24718513],\n",
       "              [0.24725026],\n",
       "              [0.23261815],\n",
       "              [0.2471972 ],\n",
       "              [0.24723697],\n",
       "              [0.2323229 ],\n",
       "              [0.23140587],\n",
       "              [0.2477279 ],\n",
       "              [0.23209357],\n",
       "              [0.24751735],\n",
       "              [0.24730821],\n",
       "              [0.24750674],\n",
       "              [0.23272803],\n",
       "              [0.24727444],\n",
       "              [0.24727756],\n",
       "              [0.24726333],\n",
       "              [0.24735498],\n",
       "              [0.23130915],\n",
       "              [0.24735644],\n",
       "              [0.2326549 ],\n",
       "              [0.23219867],\n",
       "              [0.23273338],\n",
       "              [0.23268846],\n",
       "              [0.24724345],\n",
       "              [0.2473797 ],\n",
       "              [0.24738654],\n",
       "              [0.2314789 ],\n",
       "              [0.24723507],\n",
       "              [0.23221129],\n",
       "              [0.24730465],\n",
       "              [0.23243599],\n",
       "              [0.232267  ],\n",
       "              [0.23234174],\n",
       "              [0.24730346],\n",
       "              [0.2472916 ],\n",
       "              [0.24730274],\n",
       "              [0.2325149 ],\n",
       "              [0.24734567],\n",
       "              [0.24727803],\n",
       "              [0.24726497],\n",
       "              [0.23281087],\n",
       "              [0.24736117],\n",
       "              [0.23239669],\n",
       "              [0.23156339],\n",
       "              [0.2475236 ],\n",
       "              [0.24708891],\n",
       "              [0.2476187 ]], dtype=float32))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lime = LimeBase(\n",
    "    func=net.apply,\n",
    "    additional_func_args={\"params\": params},\n",
    ")\n",
    "lime.attribute(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

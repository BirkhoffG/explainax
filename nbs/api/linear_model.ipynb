{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from ipynb_path import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "from jax_interpret.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_loss(x1, x2, weights=None):\n",
    "    if weights is None:\n",
    "        return jnp.mean(jnp.square(x1 - x2)) / 2.0\n",
    "    else:\n",
    "        return jnp.sum((weights / jnp.linalg.norm(weights, ord=1)) * jnp.square(x1 - x2)) / 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _init_train_fn(\n",
    "    X: jnp.ndarray, # Input data\n",
    "    y: jnp.ndarray, # Target data\n",
    "    fit_bias: bool = True, # Fit bias term\n",
    "    seed: int = 42, # Random seed\n",
    "):  \n",
    "    rng = jax.random.PRNGKey(seed)\n",
    "    n_samples, n_features = X.shape\n",
    "    rng, w_key, b_key = jax.random.split(rng, 3)\n",
    "    w = jax.random.normal(w_key, (n_features,))\n",
    "    if fit_bias:\n",
    "        b = jax.random.normal(b_key, (1,))\n",
    "    else:\n",
    "        b = jnp.zeros(1)\n",
    "    params = dict(w=w, b=b)\n",
    "    return params\n",
    "\n",
    "def calculate_loss(\n",
    "    params: Dict[str, jnp.ndarray],\n",
    "    batch: Tuple[jnp.ndarray, jnp.ndarray],\n",
    "    loss_fn: Callable,\n",
    "    weights: jnp.ndarray = None,\n",
    "    reg_term: int = None,\n",
    "    alpha: float = 1.0\n",
    "):\n",
    "    \"\"\"Calculate the loss for a batch of data.\"\"\"\n",
    "    w, b = params[\"w\"], params[\"b\"]\n",
    "    X, y = batch\n",
    "    y_pred = jnp.dot(X, w) + b\n",
    "    loss = loss_fn(y, y_pred, weights)\n",
    "    if reg_term is not None:\n",
    "        reg = jnp.linalg.norm(w, ord=reg_term)\n",
    "        loss += jnp.mean(reg) * alpha\n",
    "    return loss\n",
    "\n",
    "def sgd_train_linear_model(\n",
    "    X: jnp.ndarray, # Input data. Shape: `(N, k)`\n",
    "    y: jnp.ndarray, # Target data. Shape: `(N,)` or `(N, 1)`\n",
    "    weights: jnp.ndarray = None, # Initial weights. Shape: `(k,)`\n",
    "    lr: float = 0.01, # Learning rate\n",
    "    n_epochs: int = 100, # Number of epochs\n",
    "    batch_size: int = 32, # Batch size\n",
    "    seed: int = 42, # Random seed\n",
    "    loss_fn: Callable = l2_loss, # Loss function\n",
    "    reg_term: int = None, # Regularization term\n",
    "    alpha: float = 1.0, # Regularization strength\n",
    "    fit_bias: bool = True, # Fit bias term\n",
    ") -> Tuple[np.ndarray, np.ndarray]: # The trained weights and bias\n",
    "    \"\"\"Train a linear model using SGD.\"\"\"\n",
    "\n",
    "    @jax.jit\n",
    "    def sgd_step(params, opt_state, batch):\n",
    "        \"\"\"Perform a single SGD step.\"\"\"\n",
    "        grads = jax.grad(calculate_loss)(params, batch, loss_fn, weights, reg_term, alpha)\n",
    "        updates, opt_state = opt.update(grads, opt_state)\n",
    "        params = optax.apply_updates(params, updates)\n",
    "        return params, opt_state\n",
    "\n",
    "    # TODO: Check shapes of X and y\n",
    "    n_samples = X.shape[0]\n",
    "    params = _init_train_fn(X, y, fit_bias, seed)\n",
    "    opt = optax.sgd(lr)\n",
    "    opt_state = opt.init(params)\n",
    "    for epoch in range(n_epochs):\n",
    "        for i in range(0, n_samples, batch_size):\n",
    "            X_batch = X[i : i + batch_size]\n",
    "            y_batch = y[i : i + batch_size]\n",
    "            params, opt_state = sgd_step(params, opt_state, (X_batch, y_batch))\n",
    "    return params[\"w\"], params[\"b\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BaseEstimator:\n",
    "    def __init__(self):\n",
    "        ...\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class LinearModel(BaseEstimator):\n",
    "    def __init__(\n",
    "        self,\n",
    "        bias: bool = True,\n",
    "        trainer_fn: Callable=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.fit_bias = bias\n",
    "        self.trainer_fn = sgd_train_linear_model if trainer_fn is None else trainer_fn\n",
    "    \n",
    "    def fit(\n",
    "        self, \n",
    "        X: jnp.ndarray, \n",
    "        y: jnp.ndarray,\n",
    "        weights: jnp.ndarray = None,\n",
    "        **kwargs,\n",
    "    ) -> LinearModel:\n",
    "        self.coef_, self.bias_ = self.trainer_fn(\n",
    "            X, y, weights, fit_bias=self.fit_bias, **kwargs)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Lasso(LinearModel):\n",
    "    def __init__(self, alpha: float = 1.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def fit(self, X: jnp.ndarray, y: jnp.ndarray, weights: jnp.ndarray = None, **kwargs) -> LinearModel:\n",
    "        return super().fit(X, y, weights, reg_term=1, alpha=self.alpha, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Ridge(LinearModel):\n",
    "    def __init__(self, alpha: float = 1.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def fit(self, X: jnp.ndarray, y: jnp.ndarray, weights: jnp.ndarray = None, **kwargs) -> LinearModel:\n",
    "        return super().fit(X, y, weights, reg_term=2, alpha=self.alpha, **kwargs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_regression(n_samples=500, n_features=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:jax._src.lib.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.LinearModel>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm = LinearModel()\n",
    "lm.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Lasso>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso = Lasso(alpha=0.1)\n",
    "lasso.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Ridge>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge = Ridge(alpha=0.1)\n",
    "ridge.fit(X, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from ipynb_path import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "from explainax.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = jrand.normal(jrand.PRNGKey(0), (100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def l2_loss(x1, x2, weights=None):\n",
    "    if weights is None:\n",
    "        return optax.l2_loss(x1, x2).mean()\n",
    "    else:\n",
    "        return jnp.sum((weights / optax.safe_norm(weights, 0., ord=1)) * jnp.square(x1 - x2)) / 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _init_train_fn(\n",
    "    X: jnp.ndarray, # Input data\n",
    "    y: jnp.ndarray, # Target data\n",
    "    fit_bias: bool = True, # Fit bias term\n",
    "    seed: int = 42, # Random seed\n",
    "):  \n",
    "    rng = jax.random.PRNGKey(seed)\n",
    "    n_samples, n_features = X.shape\n",
    "    rng, w_key, b_key = jax.random.split(rng, 3)\n",
    "    w = jax.random.normal(w_key, (n_features,))\n",
    "    if fit_bias:\n",
    "        b = jax.random.normal(b_key, (1,))\n",
    "    else:\n",
    "        b = jnp.zeros(1)\n",
    "    params = dict(w=w, b=b)\n",
    "    return params\n",
    "\n",
    "def calculate_loss(\n",
    "    params: Dict[str, jnp.ndarray],\n",
    "    batch: Tuple[Array, Array, Array],\n",
    "    loss_fn: Callable,\n",
    "    reg_term: int = None,\n",
    "    alpha: float = 1.0\n",
    "):\n",
    "    \"\"\"Calculate the loss for a batch of data.\"\"\"\n",
    "    w, b = params[\"w\"], params[\"b\"]\n",
    "    X, y, weights = batch\n",
    "    y_pred = jnp.dot(X, w) + b\n",
    "    loss = loss_fn(y, y_pred, weights)\n",
    "    if reg_term is not None:\n",
    "        reg = optax.safe_norm(w, 0., ord=reg_term)\n",
    "        loss += jnp.mean(reg) * alpha\n",
    "    return loss\n",
    "\n",
    "def sgd_train_linear_model(\n",
    "    X: jnp.ndarray, # Input data. Shape: `(N, k)`\n",
    "    y: jnp.ndarray, # Target data. Shape: `(N,)` or `(N, 1)`\n",
    "    weights: jnp.ndarray = None, # Initial weights. Shape: `(N,)`\n",
    "    lr: float = 0.01, # Learning rate\n",
    "    n_epochs: int = 100, # Number of epochs\n",
    "    batch_size: int = 32, # Batch size\n",
    "    seed: int = 42, # Random seed\n",
    "    loss_fn: Callable = l2_loss, # Loss function\n",
    "    reg_term: int = None, # Regularization term\n",
    "    alpha: float = 1.0, # Regularization strength\n",
    "    fit_bias: bool = True, # Fit bias term\n",
    ") -> Tuple[np.ndarray, np.ndarray]: # The trained weights and bias\n",
    "    \"\"\"Train a linear model using SGD.\"\"\"\n",
    "\n",
    "    @jax.jit\n",
    "    def sgd_step(params, opt_state, batch):\n",
    "        \"\"\"Perform a single SGD step.\"\"\"\n",
    "        grads = jax.grad(calculate_loss)(params, batch, loss_fn, reg_term, alpha)\n",
    "        updates, opt_state = opt.update(grads, opt_state)\n",
    "        params = optax.apply_updates(params, updates)\n",
    "        return params, opt_state\n",
    "\n",
    "    # TODO: Check shapes of X and y\n",
    "    n_samples = X.shape[0]\n",
    "    params = _init_train_fn(X, y, fit_bias, seed)\n",
    "    opt = optax.sgd(lr)\n",
    "    opt_state = opt.init(params)\n",
    "    for epoch in range(n_epochs):\n",
    "        for i in range(0, n_samples, batch_size):\n",
    "            X_batch = X[i : i + batch_size]\n",
    "            y_batch = y[i : i + batch_size]\n",
    "            w_batch = weights[i : i + batch_size] if weights is not None else None\n",
    "            params, opt_state = sgd_step(params, opt_state, (X_batch, y_batch, w_batch))\n",
    "    return params[\"w\"], params[\"b\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BaseEstimator:\n",
    "    def __init__(self):\n",
    "        ...\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class LinearModel(BaseEstimator):\n",
    "    def __init__(\n",
    "        self,\n",
    "        intercept: bool = True,\n",
    "        trainer_fn: Callable=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.fit_bias = intercept\n",
    "        self.trainer_fn = sgd_train_linear_model if trainer_fn is None else trainer_fn\n",
    "    \n",
    "    def fit(\n",
    "        self, \n",
    "        X: jnp.ndarray, \n",
    "        y: jnp.ndarray,\n",
    "        weights: jnp.ndarray = None,\n",
    "        **kwargs,\n",
    "    ) -> LinearModel:\n",
    "        self.coef_, self.intercept_ = self.trainer_fn(\n",
    "            X, y, weights, fit_bias=self.fit_bias, **kwargs)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X: jnp.ndarray) -> jnp.ndarray:\n",
    "        return jnp.dot(X, self.coef_) + self.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Lasso(LinearModel):\n",
    "    def __init__(self, alpha: float = 1.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def fit(self, X: jnp.ndarray, y: jnp.ndarray, weights: jnp.ndarray = None, **kwargs) -> LinearModel:\n",
    "        return super().fit(X, y, weights, reg_term=1, alpha=self.alpha, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Ridge(LinearModel):\n",
    "    def __init__(self, alpha: float = 1.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def fit(self, X: jnp.ndarray, y: jnp.ndarray, weights: jnp.ndarray = None, **kwargs) -> LinearModel:\n",
    "        return super().fit(X, y, weights, reg_term=2, alpha=self.alpha, **kwargs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_regression(n_samples=500, n_features=20)\n",
    "w = np.ones(X.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 4.39208757e+01,  5.56077362e+01,  8.41533489e+01, -1.96221110e-15,\n",
       "        -3.12695418e-14,  2.31507874e-14, -3.24155956e-14,  2.37743954e+00,\n",
       "         2.76497270e+01, -1.59962711e-15,  1.29899749e-14, -4.45028592e-14,\n",
       "        -4.98098774e-14, -8.17722613e-14,  9.44247846e+01,  8.93984093e+01,\n",
       "         5.23727300e+01, -8.15458812e-14,  7.31298648e+01,  4.14151921e+00]),\n",
       " -1.0658141036401503e-14)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sk_lm = LinearRegression()\n",
    "sk_lm.fit(X, y)\n",
    "sk_lm.coef_, sk_lm.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([ 4.3920807e+01,  5.5607498e+01,  8.4153252e+01,  2.5172596e-04,\n",
       "        -2.4921859e-05, -9.8353492e-05, -2.5988952e-04,  2.3774581e+00,\n",
       "         2.7649561e+01,  3.1019867e-04, -2.3504299e-04,  2.3154756e-04,\n",
       "         9.7808908e-05, -6.8332774e-05,  9.4424484e+01,  8.9398209e+01,\n",
       "         5.2372467e+01, -2.0462631e-04,  7.3129379e+01,  4.1418076e+00],      dtype=float32),\n",
       " Array([1.1218661e-05], dtype=float32))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm = LinearModel()\n",
    "lm.fit(X, y)\n",
    "lm.fit(X, y, w)\n",
    "lm.coef_, lm.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(sk_lm.coef_, lm.coef_, atol=5e-4)\n",
    "assert np.allclose(sk_lm.intercept_, lm.intercept_, atol=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([ 4.3809612e+01,  5.5486099e+01,  8.4031708e+01, -2.9320540e-04,\n",
       "         4.6559394e-04, -9.0057432e-04,  1.1306580e-03,  2.2790406e+00,\n",
       "         2.7526842e+01, -1.0292386e-03, -5.8906851e-04,  1.5921631e-03,\n",
       "        -1.3546057e-03, -5.7649292e-04,  9.4333282e+01,  8.9276718e+01,\n",
       "         5.2267292e+01,  3.2355968e-04,  7.2997238e+01,  4.0658412e+00],      dtype=float32),\n",
       " Array([0.00556847], dtype=float32))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso = Lasso(alpha=0.1)\n",
    "lasso.fit(X, y)\n",
    "lasso.fit(X, y, w)\n",
    "lasso.coef_, lasso.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([ 4.3895161e+01,  5.5568810e+01,  8.4103256e+01,  1.9272733e-03,\n",
       "        -1.3708844e-03, -2.7952294e-04, -5.8420287e-03,  2.3761270e+00,\n",
       "         2.7629921e+01,  8.9110909e-03, -1.1335424e-03,  3.1227635e-03,\n",
       "         1.2426455e-04, -9.2288008e-04,  9.4378990e+01,  8.9345421e+01,\n",
       "         5.2339722e+01, -6.7419285e-04,  7.3078979e+01,  4.1462922e+00],      dtype=float32),\n",
       " Array([0.00056191], dtype=float32))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge = Ridge(alpha=0.1)\n",
    "ridge.fit(X, y)\n",
    "ridge.fit(X, y, w)\n",
    "ridge.coef_, ridge.intercept_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
